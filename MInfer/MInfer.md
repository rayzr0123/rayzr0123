### MInference

#### 1. 摘要

大型语言模型 (LLM) 推理的计算挑战仍然是其广泛部署的重大障碍，特别是随着提示长度不断增加。由于注意力计算的二次复杂度，8B LLM 在单个 A100 GPU 上处理 1M 个 token 的提示（即预填充阶段）需要 30 分钟。当应用于长上下文法学硕士时，现有的加速预填充的方法通常无法保持可接受的准确性或效率。为了解决这一差距，我们引入了MInference（百万tokens推理），这是一种稀疏计算方法，**旨在加速长序列处理的预填充**。具体来说，我们确定了长上下文注意力矩阵中的**三种独特模式——A 形、垂直斜线和块稀疏**——可用于在 GPU 上进行高效稀疏计算。我们离线确定每个注意力头的最佳模式，并在推理过程中根据分配的模式动态构建**稀疏索引**。借助模式和稀疏索引，我们通过**优化的 GPU 内核**执行高效的稀疏注意力计算，以显着减少长上下文 LLM 预填充阶段的延迟。我们提出的技术可以直接应用于现有的LLM，无需对预训练设置进行任何修改或进行额外的微调。通过评估各种下游任务，包括 InfiniteBench、RULER、PG-19 和 Needle In A Haystack，以及模型包括 LLaMA-3-1M、GLM-4-1M、Yi-200K、Phi-3-128K，和 Qwen2-128K，我们证明了 MInference 可以有效地将 A100 上预填充的推理延迟降低多达 10 倍，同时保持准确性。



#### 2. 前言

大型语言模型（LLM）已经进入长上下文处理时代，其中一些模型支持从 128K 到 10M 标记的上下文窗口 [ Gra24 、 RST+24 、 LYZA24 、 YCL+24 、 AJA+24 、 DA24 ]。这些扩展的上下文窗口使LLM能够解锁大量复杂的现实应用程序，例如存储库级代码理解[BSK+23、JYW+23、POC+23]、长文档问答[CPG] +23，LZD+24]，极端标签上下文学习[LZD+24]，以及长视野代理任务[Wen23]



然而，由于注意力的二次复杂度，模型可能需要几分钟的时间来处理输入提示（即预填充阶段），然后开始产生第一个令牌，这导致了不可接受的 Time To First Token 体验，从而极大地阻碍了长语境LLM的广泛应用。如图 2a 所示，当在单台 A100 机器上提供 LLaMA-3-8B 时，该模型会在提示 300K token的情况下让用户等待 6 分钟才能完成预填充阶段，并且该数字增加到 30 分钟提示 1M token。**自注意力计算的开销超过了总预填充延迟的 90%**，这使其成为 LLM 长上下文处理的主要瓶颈。先前的研究表明，**注意力矩阵是高度稀疏的**[LQC+22，DSY24]，这导致了Longformer [BPC20]和BigBird [ZGD+20]等固定稀疏注意力方法的发展。然而，**先前的研究也指出，不同输入的注意力分布存在显着差异**[LCW21，LQC+22]。这种动态性质阻止了先前的稀疏方法在没有昂贵的训练或微调的情况下直接用于长上下文LLM。但是，**如果可以有效地在线预测动态稀疏注意力模式，则可以通过仅计算注意力权重的最重要部分来显着减少长上下文 LLM 的预填充延迟。**【注意力矩阵稀疏  +  差异 ——> 预测】



基于这个想法，我们提出了 MInference，这是一种能够减少注意力计算中 95% 的 FLOP 的技术，通过**动态稀疏注意力**显着加速长上下文 LLM 推理的**预填充阶段**。与现有的动态稀疏注意力方法引入大量计算开销来估计具有低秩隐藏维度的注意力模式[LQC+22，RCHG+24]不同，我们的方法是专为长上下文场景而设计的，估计开销最小。具体来说，我们进行了广泛的分析，并确定了长上下文 LLM 中稀疏注意力的三种常见模式：**A 形模式、垂直斜杠模式和块稀疏模式**。基于这些发现，我们引入了**一种内核感知搜索方法来为每个头分配最佳的注意力模式**。重要的是，我们没有执行先前研究中的固定注意mask，而是执行有效的**在线**近似，**根据分配的模式和特定输入为每个头构建动态稀疏mask**。例如，为了在一个 Vertical-Slash 头上为特定提示构建动态稀疏掩码，我们使用由最后一个 last_q 查询和关键向量（即 Q[−last_q:] 和 K）组成的部分注意力权重来估计注意力矩阵上全局垂直线和斜线最重要的索引。对于块稀疏头，我们对 64 个块中的查询向量和键向量执行均值池化，并计算块级注意力权重以确定最重要的块，从而获得块稀疏动态掩码。获得动态稀疏掩码后，使用我们**针对上述三种稀疏模式开发三个优化的 GPU 内核**。**这些内核基于动态稀疏编译器 PIT [ZJZ+23]、Triton [TKC19] 和 FlashAttention [Dao24]**，可以极其高效地计算动态稀疏注意力。



在各种长上下文 LLM 上进行了广泛的实验，包括 LLaMA-3-8B-1M [ Gra24]、GLM-4-9B-1M [GZX+24] 和 Yi-9B-200K [YCL+24] 的跨基准测试上下文长度超过 1M token，例如 InfiniteBench [ ZCH+24]、RULER [HSK+24]、Needle In A Haystack [Kam23] 和 PG-19 [ RPJ+20 ]。 Needle In A Haystack 也在 Phi-3-Mini-128K [AJA+24] 和 Qwen-2-7B-128K [BBC+23] 上进行了测试。结果表明，MInference 在单个 A100 上使用 LLaMA-3-8B 将 1M 上下文的预填充阶段速度提高了 10 倍，将每次提示的延迟从 30 分钟减少到 3 分钟，同时保持或提高了准确性。

![image-20240902132616184](.\image-20240902132616184.png)



#### **3 Attention Heads: Dynamic, Sparse, and Characteristic**

**3.1 Attention is Dynamically Sparse**

大小为 128k × 128k 的注意力矩阵，仅保留前 4k 列，可以回忆起总注意力的 96.8%。换句话说，尽管每个token正在处理很长的序列，但它只关注有限数量的token。

另一方面，尽管注意力矩阵的稀疏性质在不同的输入之间共享，但稀疏模式的确切分布是高度动态的。也就是说，给定位置的token仅关注自注意力序列的子集，并且它关注的确切token**高度依赖于上下文**，并且在不同的提示中存在显着差异。这种活力已在先前的研究中得到了数学证明[LCW21，LCW23]。如图 2c 所示，如果我们将图 2b 中的前 4k 列应用到另一个 128k 的提示上，注意力的召回率将大幅下降至 83.7%。

**3.2 Attention Sparsity Exhibits Patterns**

尽管注意力矩阵的稀疏分布是动态的，但先前的工作[XTC+24，HWP+24]已经表明它们在二维空间中表现出某些**模式**，例如空间聚类。通过对不同长度和任务的长上下文提示的分析，我们将注意力稀疏模式归纳为三类：**A-shape, Vertical-Slash (VS), and Block-Sparse patterns**
![image-20240902133406609](.\image-20240902133406609.png)对于不同的提示和任务，同一头的模式相对一致，但**稀疏索引是动态变化的**。

**A形模式**：此类头部的注意力权重集中在初始标记和局部窗口[XTC+24，HWP+24]上，表现出相对较高的稳定性。**垂直斜线（VS）模式**：注意力权重集中在特定标记（垂直线）和固定间隔的标记（斜线）上。该图案中垂直线和斜线的位置随着上下文内容动态变化，并表现出一定的稀疏性，使得它们难以被局部窗口和A形图案所包围。
**块稀疏模式** ：这种稀疏模式是最动态的，表现出更分散的分布。尽管具有动态性，注意力权重仍然保持了空间聚类的一些特征，我们将其识别为块稀疏模式。我们分析了 128k 提示内非零注意力权重与其前 k 个最近的非零邻居之间的距离，如图 3b 所示。结果表明，跨层和头，最近的非零值之间的距离通常集中在 5 左右，这**表明注意力权重存在很强的空间聚类**。



这三种模式的要点在于，我们可以利用它们对长上下文 LLM 中的注意力矩阵执行高效的稀疏计算。在图 3c 中，我们测试了我们识别的模式在 GPU 计算成本 (FLOP) 有限的情况下检索注意力分数的效率。首先，注意力头被标记为稀疏模式之一。然后我们证明我们的模式比其他稀疏方法 [RCHG+24、XTC+24、PPJF24] 显着更有效。具体来说，在相同数量的 FLOP 下，我们的模式在注意力分数上实现了显着更高的召回率，这可能会带来更高的准确性。例如，以前的 Top-K 方法 [RCHG+24、XTC+24、PPJF24] 与块稀疏模式作斗争，因为它们专注于全局的特定标记，而我们的模式可以更有效、更准确地检索注意力分数。



#### 4. MInference 1.0

我们提出 MInference 来加速长上下文 LLM 的预填充阶段，包括三个步骤：1）每个头的离线注意力模式识别； 2) 动态构建稀疏索引 w.r.t.模式； 3）优化GPU内核的稀疏注意力计算。

![image-20240902134320719](.\image-20240902134320719.png)

**4.1 Problem Formulation**
当通过稀疏注意力计算加速长上下文LLM的预填充阶段时，注意力矩阵可以表述如下：
                                          <img src=".\image-20240902134449710.png" alt="image-20240902134449710" style="zoom:80%;" />
其中 Mi,j ∈ {0, 1} 表示注意力矩阵的第 i, j 项的动态稀疏掩码。这里，c是一个很大的常数，例如1e5，确保Mi,j = 0的不太重要的注意力权重在softmax之后具有接近零的值，即Ai,j ≈ 0。

动态稀疏注意力系统的目标是以**最小的开销**实现更大的加速，同时**保留尽可能多的注意力权重**。形式上，这可以表示为：
                                                             <img src=".\image-20240902134732468.png" alt="image-20240902134732468" style="zoom:80%;" />

其中 t_sparse 和 t_overhead 分别表示动态稀疏**注意力计算**和近似动态稀疏**模式估计**所花费的时间。

**4.2 Speedup of Long-context LLM Inference via Dynamic Sparse Attention**

**Kernel-Aware Optimal Sparse Pattern Search**

为了在有限的 FLOPs 预算下实现最佳精度，我们提出了一种离线内核感知最优稀疏模式搜索方法。在这一步中，我们确定每个注意力头将使用哪种稀疏模式，以及实际计算中该模式的最佳设置（例如，VS模式中垂直/斜线的数量；或BS 模式）。如算法 1 所示，我们**首先根据每个模式的目标 FLOP 创建搜索空间，确保所有潜在候选者（即具有不同设置的不同模式）具有相似的计算成本。这里的内核感知表示计算成本反映了 GPU 内核中的真实 FLOP**，而不是概念估计，这对于实现最佳加速至关重要。



接下来，我们通过参考示例遍历搜索空间来决定最佳模式和设置。具体来说，**我们在搜索最佳模式时使用注意力输出的recall作为客观标准**。这种方法利用 FlashAttention [Dao24] 来减少 GPU 内存开销，并合并来自 V 矩阵的信息，从而实现最佳模式的端到端选择，从而进一步提高性能。

<img src=".\image-20240902135440408.png" alt="image-20240902135440408" style="zoom:67%;" />

**Sparsity Indices Approximation and Dynamic Sparse Attention Calculation**

在推理阶段，我们将对注意力矩阵进行在线估计，以根据分配的模式和确切的输入动态确定稀疏索引的空间分布。之后，我们使用优化的 GPU 内核进行稀疏注意力计算。我们内核的实现细节可以在附录 C.4 中找到。请注意，对于 A-shapeheads，稀疏掩码是静态的，因此构建动态掩码没有开销，只需要稀疏计算。

(i) 垂直斜线头。如算法 2 所示，由于垂直线和斜线的连续性，我们将最后一个查询向量 Q[−last_q:] 和关键向量 K 相乘，以产生估计的注意力矩阵 A^，进而用于确定垂直 i_v 和斜线i_s的索引线。【**这样做的目的是减少计算量，因为并不是所有查询向量都需要参与到计算中，特别是在注意力权重中存在明显的局部依赖时。？**】获得垂直线和斜线的稀疏索引后，我们将它们转换为稀疏格式 i_vs。使用这些稀疏索引，我们对注意力权重和注意力输出进行块稀疏计算。 

<img src=".\image-20240902140920701.png" alt="image-20240902140920701" style="zoom: 50%;" />

(ii) 块稀疏头。根据算法 3，对 Q 和 K 应用均值池化，分别获得 Q^ 和 K^。将两个矩阵相乘即可得到估计的块级注意力权重 A^。由于均值池化和矩阵乘法运算是可交换的，因此得到的注意力权重近似等于均值池化后的实际注意力权重。这使我们能够以最小的开销来近似实际注意力权重的块稀疏模式。类似地，我们构建一个稀疏索引 i_b 并用它来计算稀疏注意力权重和注意力输出。



#### 5. 实验

在本节中，我们研究两个问题：(i) MInference 的效果如何？我们在三个通用的长上下文基准测试上评估我们的方法：InfiniteBench [ZCH+24]、RULER [HSK+24]、Needle In A Haystack 任务 [Kam23]，以及长上下文语言建模任务 [RPJ+] 20]。这些基准测试涵盖长上下文 QA、多跳 QA、数学推理、聚合任务、摘要、检索任务和代码调试，使我们能够评估 MInference 在各种长上下文场景中的有效性。 (ii) MInference 的效率如何？我们深入研究端到端延迟及其细分，以评估 MInference 的效率。其他实验、延迟结果和分析可在附录 D、E 和 F 中找到。

